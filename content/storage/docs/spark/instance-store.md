# **Mount Instance Store Volumes**

When working with Spark workloads running on top of an EMR on EKS cluster, it might be useful to use instances powered by NVMe disks to improve the performance of your jobs.

In this document, we highlight two approaches to leverage NVMe disks in your workloads when using EMR on EKS. For a list of instances supporting NVMe disks, see [Instance store volumes](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html#instance-store-volumes) in the Amazon EC2 documentation.

## **Mount kubelet pod directory on NVMe disks**

The [kublet](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) service, is an agent that runs on each node of your EKS cluster. This service manages the lifetime of pod containers that are created in Kubernetes on a specific node. When a pod is launched on an instance, an ephemeral volume is automatically created for the pod, and this volume is mapped in a sub-directory within the path `/var/lib/kubelet` of the host node. This volume folder will exists for the lifetime of the K8s pod, and it will be automatically deleted once the pod ceases to exists.

In order to leverage NVMe disk attached to an EC2 node in our Spark application, we should perform the following actions on node bootstrap:

* Prepare the NVMe disks attached to the instance (format disks and create a partition)
* Mount the `/var/lib/kubelet/pods` path on the NVMe

By this doing, all local files generated by your Spark job (blockmanager data, shuffle data, etc.), will be automatically written on the NVMe disks, without the need for Spark to configure a volume path when launching the pod (driver or executor). This approach is easier to adopt rather than using a Spark pod template or mapping the device using the spark configurations, as it doesn’t require any further configuration in your job. Besides, at the job completion, data stored within the ephemeral volumes will be automatically deleted.

However, please note that if you have multiple NVMe disks attached to the instances, it will be required to create a RAID 0 setup of all the disks before mounting the `/var/lib/kubelet/pods` directory on the RAID partition. Without a RAID setup, it will not be possible to leverage all the disks capacity available on the node.

The following example shows how to create a node group in your cluster that leverages this approach. In order to prepare our NVMe disks, we can use the [eksctl](https://eksctl.io/) **preBootstrapCommands** definition while creating the node group to execute additional commands during the boot process of the instance. The script will perform the following actions:

* For instances with a single NVMe disk, format the filesystem, create a Linux partition (e.g. ext4, xfs, etc.)
* For instances with multiple NVMe disks, create a RAID 0 setup across all available disks

Once the disks have been formatted and ready to use, we can then mount the folder **/var/lib/kubelet/pods** using the filesystem just created and set the correct permissions. Below you can find an example of an eksctl configuration file to create a managed node group using this approach.

**Example**

```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: YOUR_CLUSTER_NAME
  region: YOUR_REGION

managedNodeGroups:
  - name: ng-c5d-9xlarge
    instanceType: c5d.9xlarge
    desiredCapacity: 1
    privateNetworking: true
    subnets:
      - YOUR_NG_SUBNET
    preBootstrapCommands: # commands executed as root
      - yum install -y mdadm nvme-cli
      - nvme_disks=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | awk -F'[[:space:]][[:space:]]+' '{print $1}')) && [[ ${#nvme_disks[@]} -eq 1 ]] && mkfs.ext4 -F ${nvme_disks[*]} && systemctl stop docker && mkdir -p /var/lib/kubelet/pods && mount ${nvme_disks[*]} /var/lib/kubelet/pods && chmod 750 /var/lib/docker && systemctl start docker
      - nvme_disks=($(nvme list | grep "Amazon EC2 NVMe Instance Storage" | awk -F'[[:space:]][[:space:]]+' '{print $1}')) && [[ ${#nvme_disks[@]} -ge 2 ]] && mdadm --create --verbose /dev/md0 --level=0 --raid-devices=${#nvme_disks[@]} ${nvme_disks[*]} && mkfs.ext4 -F /dev/md0 && systemctl stop docker && mkdir -p /var/lib/kubelet/pods && mount /dev/md0 /var/lib/kubelet/pods && chmod 750 /var/lib/docker && systemctl start docker
```


**Benefits**

* No need to mount the disk using Spark configurations or pod templates
* Data generated by the application, will immediately be deleted at the pod termination. Data will be also purged in case of pod failures.
* One time configuration for the node group

**Cons**

* Contention of disk resources if multiple jobs are allocated on the same node, as it’s not possible to properly re-partition resources between jobs



## **Mount NVMe disks as data volumes**

In this section, we’re going to explicitly mount each instance store volume on the host node, as mount path in the Spark pods (executors and driver).

As in the previous example, this script will automatically format the instance store volumes and create an **xfs** partition. The disks are then mounted in local folders called **/spark_data_IDX** where IDX is an integer that corresponds to the disk mounted.

**Example**

```
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: YOUR_CLUSTER_NAME
  region: YOUR_REGION

managedNodeGroups:
  - name: ng-m5d-4xlarge
    instanceType: m5d.4xlarge
    desiredCapacity: 1
    privateNetworking: true
    subnets:
      - YOUR_NG_SUBNET
    preBootstrapCommands: # commands executed as root
      - "IDX=1;for DEV in /dev/nvme[1-9]n1;do mkfs.xfs ${DEV}; mkdir -p /spark_data_${IDX}; echo ${DEV} /spark_data_${IDX} xfs defaults,noatime 1 2 >> /etc/fstab; IDX=$((${IDX} + 1)); done"
      - "mount -a"
      - "chown 999:1000 /spark_data_*"
```


In order to successfully use these volumes with Spark, it’s required to specify additional configurations to use these volumes during shuffles operations. To use a volume as local storage, the volume’s name should starts with `spark-local-dir-`.

Below an example configuration provided during the EMR on EKS job submission, that shows how to configure Spark to use 2 volumes as local storage for the job.

**Spark Configurations**

```
{
  "name": ....,
  "virtualClusterId": ....,
  "executionRoleArn": ....,
  "releaseLabel": ....,
  "jobDriver": ....,
  "configurationOverrides": {
    "applicationConfiguration": [
      {
        "classification": "spark-defaults",
        "properties": {
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-1.mount.path": "/spark_data_1",
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-1.mount.readOnly": "false",
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-1.options.path": "/spark_data_1",
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-2.mount.path": "/spark_data_2",
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-2.mount.readOnly": "false",
          "spark.kubernetes.executor.volumes.hostPath.spark-local-dir-2.options.path": "/spark_data_2"
        }
      }
    ]
  }
}
```


Please note that for this approach it is required to specify the following configurations for each volume that you want to use. (IDX is a label to identify the volume mounted)

```
# Mount path on the host node
spark.kubernetes.executor.volumes.hostPath.spark-local-dir-IDX.options.path

# Mount path on the k8s pod
spark.kubernetes.executor.volumes.hostPath.spark-local-dir-IDX.mount.path

# (boolean) Should be defined as false to allow Spark to write in the path
spark.kubernetes.executor.volumes.hostPath.spark-local-dir-IDX.mount.readOnly
```


**Benefits**

* Possible to re-partition volumes across different Spark jobs (e.g. a node group containing instances with 2 instance volumes each, can dedicate one volume per Spark job)

**Cons**

* Additional configurations required for Spark jobs to use individual volumes. This approach can be error prone if you don’t control the instance types where your jobs are launched (e.g. multiple node groups with different instance types). In this case can be useful to specify the following spark configuration to launch the executors on a specific instance type: **[spark.kubernetes.node.selector.node.kubernetes.io/instance-type](http://spark.kubernetes.node.selector.node.kubernetes.io/instance-type)**
* Data created on the volumes is automatically deleted at job completion. However, in case of failures, data might be retained in the disks.
